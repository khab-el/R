---
title: "Spark guide"
output:
  html_document:
    toc: yes
    toc_depth: 5
    toc_float: yes
---

# о sparklyr

## Обзор средств взаимодействия со Spark

*Spark* – проект *Apache*, предназначенный для кластерных вычислений, представляет собой быструю и универсальную среду для обработки данных, в том числе и для машинного обучения. *Spark* также имеет *API* и для *R* (пакет *SparkR*), который входит в сам дистрибутив *Spark*. Но, помимо работы с данным *API*, имеется еще два альтернативных способа работы со *Spark* в *R*. Итого, существует три различных способа взаимодействия с кластером *Spark*.

Помимо официального пакета *SparkR*, возможности в машинном обучении которого слабы (в версии 1.6.2 всего одна модель, в версии 2.0.0 их четыре), имеется еще два варианта доступа к Spark.

Первый вариант — это использование продукта от *Microsoft* — *Microsoft R Server for Hadoop*, в который недавно была интегрирована поддержка *Spark*. Использование данного продукта достаточно хорошо описано в официальной документации к *HDInsight* на сайте *Microsoft*.

Второй вариант - это использование нового пакета *sparklyr*, который пока находится в стадии разработки. Этот продукт разрабатывается под эгидой *RStudio* — компании, под крылом которой выпущены одни из самых полезных и необходимых пакетов – *knitr, ggplot2, tidyr, lubridate, dplyr* и другие, поэтому этот пакет может стать еще одним лидером. Пока данный пакет слабо документирован, так как еще официально не выпущен.

На основе документации каждого из этих способов работы со *Spark*, представлена следующая таблица *(Табл. 1)* с обобщенными функциональными возможностями каждого из способов (также *SparkR 2.0.0*, в котором возможностей стало чуть больше). 
![*Таблица 1. Обзор возможностей разных способов взаимодействия с Spark*](C:\Users\eldar.khabibulin\Desktop\R mrm\guide\tbl.jpg "Табл. 1")

Как видно из таблицы, нет ни одного средства в полной мере реализующие необходимые потребности «из коробки», но пакет *sparklyr* выгодно отличается от *SparkR* и *R Server*. Основные его достоинства – чтение *csv, json, parquet* файлов из *hdfs*. Полностью совместимый с *dplyr* синтаксис манипулирования данными – включающий в себя операции фильтрации, выбора колонок, агрегирующие функции, возможности выполнять слияние данных, модификацию имен колонок и многое другое. В отличии от *SparkR* или *R server for Hadoop*, где некоторые из этих задач либо не выполняются, либо выполняются очень неудобно.

## Немного слов о Yarn

*Yarn* - это кластер менеджер, который можно использовать для распределения задач *Spark* по всему кластеру. 
Когда пишется какой-то код, работающий, например, с *jdbc* или *ORM*, фактически что происходит? Есть машина, которая запускает *Java*-процесс, и когда в этом процессе бежит код, обращающийся к базе данных, все данные вычитываются из БД и перегоняется туда, где работает этот *Java*-процесс. Когда мы говорим про *big data*, это сделать невозможно, потому что данных слишком много — это неэффективно и у нас  образуется горлышко бутылки. Кроме того, *data* и так уже распределенная и изначально находится на большом количестве машин, поэтому правильнее не *data* тянуть к этому  процессу, а код распределять на те машины, на которых мы хотим эту «дату» обрабатывать. Соответственно, это происходит параллельно на многих машинах, мы задействуем неограниченное количество ресурсов, и вот здесь нам нужен кластер-менеджер, который будет координировать эти процессы.
![*Spark*](C:\Users\eldar.khabibulin\Desktop\R mrm\guide\yrn.png)

*Driver* — это *main*, который запускается на отдельной машине (не имеющей отношения к кластеру). Когда мы сабмитим наше *Spark*-приложение, мы обращаемся к *Yarn*, который является ресурс-менеджером. Мы ему говорим, сколько *worker*-ов задействовать под наш *Java*-процесс (например, 3). Он из кластерных машин выбирает одну машину, которая будет называться *Application Master*. Ее задача — получить код и найти в кластере три машины для его выполнения. Находятся три машины, поднимаются три отдельных Java-процесса (три *executor*-а), где запускается код. Потом это все возвращается *Application Master*, и в конечном итоге он возвращает это напрямую на *Driver*, если мы хотим результат операции над *big data* получить обратно туда, откуда код вышел.

# Подключение

## Подключение к локальному экземпляру Spark
Локальный режим - отличный способ научиться и поэкспериментировать со *Spark*. Он также обеспечивает удобную среду разработки для анализа, создания отчетов и приложений, которые вы планируете развернуть в многоузловом *Spark-кластере*.

В приведенном ниже коде подключение производится к локальному экземпляру *Spark* через функцию *spark_connect*:
```
library(sparklyr)
sc <- spark_connect(master = "local")
```
Возвращенное соединение *Spark* (*sc*) обеспечивает удаленное управление источником данных с помощью пакета *dplyr* на *Spark* кластере.
Подключение к *Spark* можно настроить, установив значения определнных свойств *Spark*. В *sparklyr* свойоства *Spark* могут быть заданы с помощью аргумента *config* в функции *spark_connect*. Ниже приведены рекомендуемые свойства *Spark* для установки подключения через *R*:

* sparklyr.cores.local - количество ядер, которые будут задействованы *Spark*. По умолчанию используются все доступные ядра. Не обязательно устанавливать это свойство, если нет причин использовать меньше ядер, чем доступно для данного сеанса *Spark*.
* sparklyr.shell.driver-memory - объем оперативной памяти. По умолчанию - это весь доступный объем ОП на компьютере, минус то, что потребуется для операций OS.
* spark.memory.fraction - по умолчанию установлено 60% запрошенной памяти для каждого исполняемого действия.

**Пример подключения**
```
conf <- spark_config() # Load variable with spark_config()

conf$`sparklyr.cores.local` <- 4
conf$`sparklyr.shell.driver-memory` <- "16G"
conf$spark.memory.fraction <- 0.9

sc <- spark_connect(master = "local", 
                    version = "2.1.0",
                    config = conf)
```

## Подключение к удаленным кластерам Spark

Пример подключения, когда в качестве значения для главного аргумента в *spark_connect()* указано *yarn-client*:
```
sc <- spark_connect(master = "yarn-client", 
                    spark_home = "/opt/cloudera/parcels/SPARK2/lib/spark2",
                    method = "shell",
                    app_name = "spark-rstudio",
                    config = list(
                      "sparklyr.csv.embedded" = "^1.*",
                      "spark.driver.memory" = "24G",
                      "spark.executor.cores" = 4,
                      "spark.executor.instances" = 12,
                      "spark.driver.memory" = "12G" ,
                      "spark.driver.maxResultSize" = "12G",
                      "spark.kryoserializer.buffer.max" = "1G",
                      "spark.files.fetchTimeout" = "30000s",
                      "sparklyr.shell.driver-memory" = "12G",
                      "sparklyr.shell.executor-memory" = "12G",
                      "spark.cassandra.input.split.size_in_mb" = 254,
                      "spark.yarn.executor.memoryOverhead" = "4G",
                      "spark.task.reaper.enabled" = FALSE,
                      "spark.sql.parquet.binaryAsString" = TRUE,
                      "spark.rpc.message.maxSize" = 256,
                      "spark.driver.port" = 20001,
                      "spark.driver.blockManager.port" = 20002,
                      "spark.ui.port" = 4041,
                      "spark.driver.bindAdress" = "172.17.0.3",
                      "spark.driver.host" = "10.64.24.217"
                    ))
```

В настоящее время нет хорошего способа централизованно управлять пользовательскими подключениями к службе *Spark*. Существуют некоторые ограничения и настройки, которые могут применяться, но в большинстве случаев конфигурации подключения, пользователь *R* должен настроить сам.

Страница [Running on YARN](https://spark.apache.org/docs/latest/running-on-yarn.html "Running on YARN") на официальном веб-сайте *Spark* - это лучшее место для начала настройки параметров конфигурации. Администраторы кластеров и пользователи могут извлечь выгоду из этого документа. 

# Чтение и запись данных

Вы можете читать данные в *Spark DataFrames*, используя следующие функции:

**Функция**        | **Описание**
:----------------  | :----------------------------------
spark_read_csv     | Считывает CSV-файл и предоставляет источник данных, совместимый с dplyr
spark_read_json    | Читает файл JSON и предоставляет источник данных, совместимый с dplyr
spark_read_parquet | Читает файл паркета и предоставляет источник данных, совместимый с dplyr

Независимо от формата ваших данных, *Spark* поддерживает чтение данных из разных источников данных. К ним относятся данные, хранящиеся в HDFS (протокол hdfs: //), Amazon S3 (протокол s3n: //) или локальные файлы, доступные для рабочих узлов Spark (файл: // протокол)

Пример чтения файла паркета в текущей сессии *Spark*:
```
new_tbl <- spark_read_parquet(sc=sc, name = "tbl", path = "hdfs://bda1-cluster-ns/data/adwh/prod_rep_sbx/modeling_abt_pacl_small_2/", memory = T, overwrite = T)
```
Aргумент *memory* для функции *spark_read_* отвечает за то, будут ли данные загружены в память как RDD. Установка его в FALSE означает, что Spark по существу отобразит файл, но не сделает его копию в памяти. Это заставляет команду *spark_read_* работать быстрее, но компромисс заключается в том, что любые операции преобразования данных будут занимать гораздо больше времени.

Убедимся в том, что аргумент *memory* влияет на работу *spark_read_*, посмотрим на скорость выполнения команды с раным значением этого аргумента, а также на скорость выполнения команды по выводу количества строчек:
```
#read parquet file with memory FALSE
time_start <- Sys.time()
new_tbl <- spark_read_parquet(sc=sc, name = "tbl", path = "hdfs://bda1-cluster-ns/data/adwh/prod_rep_sbx/modeling_abt_pacl_small_2/", memory = F, overwrite = T)
Sys.time() - time_start

#count row
time_start <- Sys.time()
sdf_nrow(new_tbl)
Sys.time() - time_start
```

```
#read parquet file with memory TRUE
time_start <- Sys.time()
new_tbl <- spark_read_parquet(sc=sc, name = "tbl", path = "hdfs://bda1-cluster-ns/data/adwh/prod_rep_sbx/modeling_abt_pacl_small_2/", memory = T, overwrite = T)
Sys.time() - time_start

#count row
time_start <- Sys.time()
sdf_nrow(new_tbl)
Sys.time() - time_start
```

Часто полезно сохранять результаты вашего анализа или таблицы, которые были сгенерированы в вашем Spark-кластере, в постоянное хранилище. Лучшим вариантом во многих сценариях является запись таблицы в файл Parquet с использованием функции *spark_write_parquet*. Например:

```
spark_write_parquet(new_tbl, "hdfs://bda1-cluster-ns/data/adwh/prod_rep_sbx/new")
```
Вы также можете сохранять данные в CSV формате или JSON, используя функции *spark_write_csv* и *spark_write_json*.

# Обработка данных

## Основные команды dplyr на примере датасета nycflights13

Ниже продемонстрированы некторые из основных глаголов обработки данных пакета *dplyr* над данными из пакета *nycflights13*. Этот пакет содержит данные для всех 336 776 рейсов, вылетавших из Нью-Йорка в 2013 году. Он также включает полезные метаданные о аэропортах, погоде и самолетах. Данные поступают из Бюро статистики транспорта США и документируются в журнале *nycflights13*.

Подключитесь к кластеру, установите пакет *nycflights13*, если он еще не установлен, и скопируйте данные рейсов с помощью функции *copy_to*. Данные полетов из *nycflights13* удобны для демонстраций *dplyr*, потому что они небольшие, но на практике большие данные редко копируются непосредственно из объектов R.
```{r}
#install.packages("nycflights13")
library(sparklyr)
library(nycflights13)
library(ggplot2)
library(dplyr)

sc <- spark_connect(master="local")
flights <- copy_to(sc, flights, "flights", overwrite = T)
airlines <- copy_to(sc, airlines, "airlines", overwrite = T)
src_tbls(sc)
```
Команда *src_tbls(sc)* возвращает названия всех таблиц, находящихся в *Spark*.

Глаголы - это команды *dplyr* для управления данными. При подключении к *Spark DataFrame, dplyr* переводит команды в операторы *Spark SQL*. В удаленных источниках данных используются те же пять глаголов, что и в локальных источниках данных. Вот пять глаголов с соответствующими командами *SQL*:

* select ~ SELECT
* filter ~ WHERE
* arrange ~ ORDER BY
* summarise ~ aggregators: sum, min, sd, etc.
* mutate ~ operators: +, *, log, etc.

```{r}
flights %>% select(year:day, arr_delay, dep_delay)
flights %>% filter(dep_delay > 1000)
flights %>% arrange(desc(dep_delay))
flights %>% summarise(mean_dep_delay = mean(dep_delay))
flights %>% mutate(speed = distance / air_time * 60)
```

При работе с базами данных *dplyr* пытается быть настолько ленивым, насколько это возможно:

* Он никогда не извлекает данные в R, если вы явно не попросите об этом.
* Он задерживает выполнение любой работы до последнего момента: он собирает все, что вы хотите сделать, и затем отправляет команду в базу данных за один шаг.

Например, выполните следующий код:

```{r}
c1 <- filter(flights, day == 17, month == 5, carrier %in% c('UA', 'WN', 'AA', 'DL'))
c2 <- select(c1, year, month, day, carrier, dep_delay, air_time, distance)
c3 <- arrange(c2, year, month, day, carrier)
c4 <- mutate(c3, air_time_hours = air_time / 60)
c4
```
Эта последовательность операций никогда не затрагивает базу данных. Только когда вы запрашиваете данные (например, путем печати *c4*), *dplyr* запрашивает результаты из базы данных.

Используя тот же пример сверху, вы можете написать гораздо более чистую версию:
```{r}
c4 <- flights %>%
  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %>%
  select(carrier, dep_delay, air_time, distance) %>%
  arrange(carrier) %>%
  mutate(air_time_hours = air_time / 60)
c4
```
Для завершения сессии *Spark* выполните следующую команду:
```
spark_disconnect_all()
```


**Группировка данных**

Функция *group_by* из *dplyr* соответствует оператору GROUP BY в SQL.
```
new_tbl %>%
  group_by(report_dt) %>%
  summarize(count = n(), mean = mean(cc_curr_cnt))
```

**Сбор данных**

Вы можете копировать данные из *Spark* в память *R*, используя команду *collect()*.
```
split_tbl <- new_tbl %>%
  filter(children_cnt > 2, education_level_cd %in% c('3', '2')) %>%
  arrange(report_dt) %>%
  mutate(age_month = age * 12)
  
split <- collect(sample_n(split_tbl, 100))
```
При получении команды *collect ()*, *Spark* выполнит инструкцию *SQl* и отправит результат обратно в *R* в виде датасета.

**Оконные функции**

*dplyr* поддерживает оконныые функции *Spark SQL*. Оконные функции используются совместно с *mutate* и *filter* для решения широкого круга проблем. Вы можете сравнить синтаксис *dplyr* с запросом, который он сгенерировал, используя *dbplyr :: sql_render ()*.
```
# Rank each flight within a daily
ranked <- new_tbl %>%
  group_by(report_dt) %>%
  select(inc_salary_hy1_amt) %>% 
  mutate(rank = rank(desc(inc_salary_hy1_amt)))
dbplyr::sql_render(ranked)
```

**Объединение таблиц**

В *dplyr* существует три семейства глаголов, которые работают с двумя таблицами одновременно:

* Мутирующие объединения, которые добавляют новые переменные в одну таблицу из совпадающих строк в другой.
* Фильтрация соединений, которые фильтруют наблюдения из одной таблицы в зависимости от того, соответствуют ли они наблюдению в другой таблице.
* Установите операции, которые объединяют наблюдения в наборах данных, как если бы они были элементами набора.

Все two-table глаголы работают аналогично. Первые два аргумента - это x и y и представляют таблицы для объединения. Вывод всегда представляет собой новую таблицу с тем же типом, что и x.

Прочитаем две таблички из *Hadoop*
```
part1 <- spark_read_parquet(sc, "part1", "hdfs://bda1-cluster-ns/data/adwh/prod_rep_sbx/part1", memory = T, overwrite = T)
part2 <- spark_read_parquet(sc, "part1", "hdfs://bda1-cluster-ns/data/adwh/prod_rep_sbx/part2", memory = T, overwrite = T)
```
Следующие команды эквивалентны:
```
part1 %>% left_join(part2)
```
```
part1 %>% left_join(part2, by = "individual_id")
```
```
part1 %>% left_join(part2, by = c("individual_id", "individual_id"))
```

**Выделение части данных**

Вы можете использовать *sample_n ()* и *sample_frac ()* для получения произвольной выборки строк: *sample_n ()* используется для выделения фиксированного числа строк, а *sample_frac ()* для фиксированной доли строк.
```
c1 <- sample_n(new_tbl, 100)
sdf_nrow(c1)
```
```
c1 <- sample_frac(new_tbl, 0.1)
sdf_nrow(c1)
```

**Регистрация полученного датасета в Spark**

*sdf_register* зарегистрирует полученный *Spark SQL* в *Spark.* Результаты будут отображаться в виде таблицы, называемой *flight_spark.* Но одноименная таблица все еще не будет загружена в память Spark.
```
sdf_register(ranked, "ranked_spark")
```

# Spark ML

## Алгоритмы реализованные в sparklyr

*sparklyr* обеспечивает привязку к распределенной библиотеке машинного обучения *Spark*. В частности, *sparklyr* позволяет вам получить доступ к процедурам машинного обучения, предоставляемым пакетом *spark.ml*. Вместе с интерфейсом *dplyr* от *sparklyr* вы можете легко создавать и настраивать рабочие процессы машинного обучения на *Spark*, организованные полностью внутри *R*.

*sparklyr* предоставляет три семейства функций, которые вы можете использовать при обучении *Spark*:

* Алгоритмы машинного обучения для анализа данных *(ml_)*
* Функциональные трансформаторы для управления отдельными функциями *(ft_)*
* Функции для управления *Spark DataFrames* *(sdf_)*

Аналитический рабочий процесс с *sparklyr* может состоять из следующих этапов.

1. Выполнить *SQL*-запросы через интерфейс *sparklyr* *dplyr*
2. Использовать семейство функций *sdf_* и *ft_* для создания новых столбцов или разделения вашего набора данных
3. Выбрать подходящий алгоритм машинного обучения из семейства функций *ml_* для моделирования ваших данных
4. Проверьте качество вашей модели и используйте ее для создания прогнозов с новыми данными
5. Соберите результаты для визуализации и дальнейшего анализа в *R*

**Алгоритм**                     | **Описание**
:------------------------------- | :-----------------------------
ml_kmeans                        | K-Means Clustering
ml_linear_regression             | Linear Regression
ml_logistic_regression           | Logistic Regression
ml_survival_regression           | Survival Regression
ml_generalized_linear_regression | Generalized Linear Regression
ml_decision_tree                 | Decision Trees
ml_random_forest                 | Random Forests
ml_gradient_boosted_trees        | Gradient-Boosted Trees
ml_pca                           | Principal Components Analysis
ml_naive_bayes                   | Naive-Bayes
ml_multilayer_perceptron         | Multilayer Perceptron
ml_lda                           | Latent Dirichlet Allocation
ml_one_vs_rest                   | One vs Rest

*Spark* предоставляет функциональные трансформаторы, облегчающие большинство преобразований данных в *Spark DataFrame*, а *sparklyr* предоставляет их в семействе функций *ft_*. Эти процедуры обычно берут один или несколько входных столбцов и генерируют новый выходной столбец, сформированный из преобразование этих столбцов.


**Функция**                  | **Описание**
:--------------------------- | :-----------------------------
ft_binarizer                 | Пороговые числовые функции для двоичной (0/1) функции
ft_bucketizer                | Преобразует столбец непрерывных функций в столбец признаков
ft_discrete_cosine_transform | Преобразует вещественную последовательность длины NN во временной области в другую длину NN-вещественную последовательность в частотной области
ft_elementwise_product       | Умножает каждый входной вектор на предоставленный вектор, используя умножение по элементам
ft_index_to_string           | Отображает столбец индексов обратно в столбец, содержащий исходные метки в виде строк
ft_quantile_discretizer      | Принимает столбец с непрерывными функциями и выводит столбец со строгими категориальными функциями
sql_transformer              | Осуществляет преобразования, которые определяются оператором SQL
ft_string_indexer            | Кодирует строковый столбец меток в столбец индексов 
ft_vector_assembler          | Объединение заданного списка столбцов в один векторный столбец

## Пример линейной регрессии на локальном Spark

Простой пример, чтобы продемонстрировать использование алгоритмов машинного обучения *Spark* в *R*. Мы будем использовать *ml_linear_regression*. Используем встроенный набор данных *mtcars*, мы постараемся предсказать расход топлива на автомобиль (mpg) на основе его веса (wt) И количества цилиндров, которые содержит двигатель (cyl).

Сначала установим соединение с локальным *Spark* в *R*
```{r}
library(sparklyr)
library(dplyr)
library(nycflights13)
library(ggplot2)

sc <- spark_connect(master="local")
```
Скопируем набор данных *mtcars* в *Spark*
```{r}
mtcars_tbl <- copy_to(sc, mtcars, "mtcars")
```
Преобразуем данные с помощью *Spark SQL*, функциональных трансформаторов и функций *DataFrame.*

1. Используем *Spark SQL* для удаления всех автомобилей с лошадиной силой менее 100
2. Используем функцию *sdf_partition* *Spark* для разделения данных о автомобилях в две группы на основе цилиндров
3. Используем функции *Spark DataFrame* для разделения данных на тестовые и обучающие

Затем применим линейную модель с использованием *Spark ML*. Модель *MPG* как функция веса и цилиндров.
```{r}
# transform our data set, and then partition into 'training', 'test'
partitions <- mtcars_tbl %>%
  filter(hp >= 100) %>%
  sdf_mutate(cyl8 = ft_bucketizer(cyl, c(0,8,12))) %>%
  sdf_partition(training = 0.5, test = 0.5, seed = 888)

# fit a linear mdoel to the training dataset
fit <- partitions$training %>%
  ml_linear_regression(mpg ~ wt + cyl)
```
```{r}
# summarize the model
summary(fit)
```

В *summary()* показано, что наша модель достаточно хороша, и что вес автомобиля, а также количество цилиндров в его двигателе станут мощными предикторами его среднего расхода топлива. (Модель предполагает, что в среднем более тяжелые автомобили потребляют больше топлива.)

Давайте используем нашу модель *Spark* для прогнозирования среднего расхода топлива в нашем наборе тестовых данных и сравним прогнозируемый отклик с истинным измеренным расходом топлива. Мы построим простой график *ggplot2*, который позволит нам проверить качество наших прогнозов.
```{r}
# Score the data
pred <- sdf_predict(fit, partitions$test) %>%
  collect

# Plot the predicted versus actual mpg
ggplot(pred, aes(x = mpg, y = prediction)) +
  geom_abline(lty = "dashed", col = "red") +
  geom_point() +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_fixed(ratio = 1) +
  labs(
    x = "Actual Fuel Consumption",
    y = "Predicted Fuel Consumption",
    title = "Predicted vs. Actual Fuel Consumption"
  )
```

Несмотря на простоту, наша модель, похоже, довольно неплохо предсказывает средний расход топлива на автомобиль.


## Пример линейной регрессии на удаленном кластере


Прочитаем *parquet* файл хранящийся в *Hadoop*
```
new_tbl <- spark_read_parquet(sc=sc, name = "tbl", path = "hdfs://bda1-cluster-ns/data/adwh/prod_rep_sbx/modeling_abt_pacl_small_2/", memory = T, overwrite = T)
```
Посчитаем количество столбцов, содержащихся в таблице
```
length(colnames(new_tbl)))
```
Выберем только числовые столбцы
```
preview <- new_tbl %>% sample_n(10) %>% collect

variable <- colnames(new_tbl)
type <- as.data.frame(sapply(preview, class))
a <- c()
for (i in 1:length(type)) {
  a[i] <- levels(type[[i]]) == "integer"
}
a <- variable[a == T]

preview <- new_tbl %>% select(a, target_flg)
```
Используем *Spark SQL* для обработки данных и разделим данные на тестовые и обучающие
```
partitions <- preview %>%
  mutate(target_flg = as.integer(target_flg)) %>%
  sdf_partition(train = 0.4, test = 0.6, seed = 222)
```
Перенесем обучающую выборку в *R* для обучения модели
```
#fit a linear model to the training dataset
fit <- partitions$train %>% ml_linear_regression(response = "target_flg"0", features = c("", "", ))
```
Посмотрим на нашу модель
```
summary(fit)
```

```
# Score the data
predicted <- predict(fit_model, partitions$test) 

actual <- (partition$test %>%
            select(target_flg) %>%
            collect())

data <- data.frame(predicted = predicted, actual = actual)
```
ROC кривая
```
library(scorecard)

perf_eva(data$target_flg, data$predicted, type = "roc", show_plot = T, seed = 123)
```
# Применение функций R к объекту Spark

## spark_apply

*sparklyr* обеспечивает поддержку для запуска произвольного *R*-кода внутри вашего *Spark Cluster* через функцию *spark_apply ()*. Это особенно полезно, когда необходимо использовать функциональные возможности, доступные только в пакетах *R* или в самом *R*, которые недоступны в *Apache Spark* или *Spark Packages*.

*spark_apply ()* применяет функцию *R* к объекту *Spark* (как правило, *Spark DataFrame*). Объекты *Spark* разбиты на разделы, чтобы их можно было распределить по кластеру. Вы можете использовать *spark_apply* с разделами по умолчанию, или вы можете определить свои собственные разделы с аргументом *group_by.* Ваша функция *R* должна вернуть еще один *Spark DataFrame*. *spark_apply* будет запускать вашу функцию *R* на каждом разделе и выводить один *Spark DataFrame*.

Функция *R*, переданная в *spark_apply*, ожидает *DataFrame* и вернет объект, который может быть передан как *DataFrame*. 
Например, применим функцию *class* через *spark_apply()*:
```
sdf_len(sc, 10, repartition = 1) %>%
  spark_apply(function(e) class(e))
```
*Spark* разбивает данные на хэш или диапазон так, чтобы датафрейм мог быть распределен по кластеру. В следующем примере мы создадим два раздела и подсчитаем количество строк в каждом разделе. Затем выведем первую запись в каждом разделе.
```
trees_tbl <- sdf_copy_to(sc, trees, repartition = 2)

trees_tbl %>%
  spark_apply(function(e) nrow(e), names = "n")
```

```
trees_tbl %>%
  spark_apply(function(e) head(e, 1))
```
Мы можем применить любую произвольную функцию к разделам в *Spark DataFrame*. Например, добавить шум к каждому столбцу. Обратите внимание, что *spark_apply* применяет функцию R ко всем разделам и возвращает один *DataFrame.*
```
trees_tbl %>%
  spark_apply(function(e) sapply(e, jitter))
```
По умолчанию *spark_apply ()* выводит имена столбцов из входного фрейма данных *Spark.* Используйте аргумент *names* для переименования или добавления новых столбцов.
```
trees_tbl %>%
  spark_apply(
    function(e) data.frame(2.54 * e$Girth, e),
    names = c("Girth(cm)", colnames(trees)))
```

Вы можете применить функцию *R* к определенным группам в своих данных. Например, предположим, что вы хотите вычислить линейную регрессию для конкретных подгрупп. Чтобы решить эту проблему, вы можете указать аргумент *group_by*. Этот пример подсчитывает количество строк в датасете *iris* по *species* и затем применяет модель линейной регресси для каждого *species*.
```
iris_tbl <- sdf_copy_to(sc, iris)

iris_tbl %>%
  spark_apply(nrow, group_by = "Species")
```

```
iris_tbl %>%
  spark_apply(
    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,
    names = c("r_squared"),
    group_by = "Species")
```

С помощью *spark_apply ()* вы можете использовать любой пакет *R* внутри Spark. Например, вы можете использовать пакет *broom* для создания аккуратного кадра данных из линейной регрессии.
```
spark_apply(
  iris_tbl,
  function(e) broom::tidy(lm(Petal_Length ~ Petal_Width, e)),
  names = c("term", "estimate", "std_error", "statistic", "p_value"),
  group_by = "Species")
```

## Создание расширений для sparklyr

*Spark* является универсальной кластерной вычислительной системой, существует много других *R*-интерфейсов, которые могут быть построены (например, интерфейсы для пользовательских конвейеров машинного обучения, интерфейсы для сторонних пакетов *Spark* и т. Д.).

Вы можете создавать собственные пользовательские интерфейсы *R* для *Spark*.

Ниже приведен пример расширения для *sparklyr*, которая вызывает функцию подсчета строки текстового файла, доступную через *SparkContext*:
```
library(sparklyr)
count_lines <- function(sc, file) {
  spark_context(sc) %>% 
    invoke("textFile", file, 1L) %>% 
    invoke("count")
}
```
Функция *count_lines* принимает аргумент *spark_connection (sc)*, который позволяет ему получить ссылку на объект *SparkContext* и, в свою очередь, вызывает метод *textFile().count()*.

Вы можете использовать эту функцию с существующим соединением *sparklyr* следующим образом:
```
library(sparklyr)
sc <- spark_connect(master = "local")
count_lines(sc, "hdfs://path/data.csv")
```
Существует несколько функций для вызова методов объектов *Java* и статических методов классов *Java*:

**Функция**   | **Описание**
:----------   | :---------------------
invoke        | Вызов метода на объекте
invoke_new    | Создает новый объект с помощью конструктора
invoke_static | Вызов статического метода для объекта

Например, чтобы создать новый экземпляр класса *java.math.BigInteger*, а затем вызвать метод *longValue()*, вы должны использовать такой код:
```
billion <- sc %>% 
  invoke_new("java.math.BigInteger", "1000000000") %>%
    invoke("longValue")
```
Вызов статического метода также прост. Например, для вызова статической функции *Math::hypot()* вы должны использовать этот код:
```
hypot <- sc %>% 
  invoke_static("java.lang.Math", "hypot", 10, 20) 
```
Следующие функции полезны для реализации функций-оболочек различных типов:

**Функция**      | **Описание**
:--------------- | :-----------------------------
spark_connection | Получает соединение Spark, связанное с объектом 
spark_jobj       | Получает Spark jobj, связанный с объектом
spark_dataframe  | Получает Spark DataFrame, связанный с объектом
spark_context    | Получает SparkContext для spark_connection
hive_context     | Получает HiveContext для spark_connection
spark_version    | Получает версию Spark (как numeric_version) для spark_connection

Использование этих функций проиллюстрировано на этом простом примере:
```
analyze <- function(x, features) {
  
  # normalize whatever we were passed (e.g. a dplyr tbl) into a DataFrame
  df <- spark_dataframe(x)
  
  # get the underlying connection so we can create new objects
  sc <- spark_connection(df)
  
  # create an object to do the analysis and call its `analyze` and `summary`
  # methods (note that the df and features are passed to the analyze function)
  summary <- sc %>%  
    invoke_new("com.example.tools.Analyzer") %>% 
      invoke("analyze", df, features) %>% 
      invoke("summary")

  # return the results
  summary
}
```
Первый аргумент - это объект, к которому можно получить доступ, используя *API*-интерфейс *Spark DataFrame* (это может быть фактическая ссылка на *DataFrame* или может быть *dplyr* **tbl**, в котором внутри есть ссылка на *DataFrame*).

После использования функции *spark_dataframe* для нормализации ссылки мы извлекаем базовое соединение *Spark*, связанное с фреймом данных, используя функцию *spark_connection.* Наконец, мы создаем новый объект *Analyzer*, вызываем его методом анализа с помощью *DataFrame* и списком функций, а затем вызываем итоговый метод по результатам анализа.

В качестве первого аргумента функции настоятельно рекомендуется принимать *spark_jobj* или *spark_dataframe*, так как в такие функции очень легко встраиваются конвейеры *magrittr*.

